# Chain of Agents Implementation

This repository contains a Python implementation of the "Chain of Agents" (CoA) approach for handling long-context tasks with Large Language Models (LLMs). CoA is based on the research paper "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks."

## Overview

Chain of Agents is a novel framework that enables multiple LLMs to collaborate sequentially to process long texts. The approach consists of:

1. **Worker Agents**: A chain of LLMs that process chunks of text sequentially, with each agent building upon the previous agent's summary.
2. **Manager Agent**: An LLM that synthesizes the final output from the accumulated information passed through the worker chain.

This implementation uses Google's Gemini 2.0 Flash model as the underlying LLM for both worker and manager agents.

## Features

- Support for both query-based tasks (e.g., question answering) and non-query-based tasks (e.g., summarization)
- Efficient chunking of long texts based on token budget
- Sequential processing of chunks with information passing between agents
- Customizable task descriptions and prompts
- Detailed metadata and logging

## Project Structure

```
chain-of-agent/
├── chain-of-agents.md       # Paper summary
├── README.md                # This file
├── requirements.txt         # Dependencies
├── src/                     # Source code
│   ├── agents/              # Worker and manager agent implementations
│   ├── chunking/            # Text chunking logic
│   ├── models/              # Model interfaces
│   └── chain_of_agents.py   # Main coordinator
└── examples/                # Example scripts
    ├── question_answering.py
    └── summarization.py
```

## Installation

1. Clone the repository
2. Install the dependencies:
   ```
   pip install -r requirements.txt
   ```
3. Set up an `.env.prod` file with your Gemini API key:
   ```
   GEMINI_API_KEY=your-api-key-here
   ```

## Usage

### Question Answering

```python
from src.chain_of_agents import ChainOfAgents

# Initialize Chain of Agents
coa = ChainOfAgents(verbose=True)

# Process a query
result = coa.query(
    text="Your long text here...",
    query="Your question here?"
)

# Get the answer
print(result["answer"])
```

### Summarization

```python
from src.chain_of_agents import ChainOfAgents

# Initialize Chain of Agents
coa = ChainOfAgents(verbose=True)

# Generate a summary
result = coa.summarize(
    text="Your long text here..."
)

# Get the summary
print(result["answer"])
```

## Examples

The repository includes example scripts for both question answering and summarization:

```
python examples/question_answering.py
python examples/summarization.py
```

## Key Considerations

- **Token Budget**: The implementation uses a default token budget of 12,000 tokens per chunk, which can be adjusted when initializing the `ChainOfAgents` class.
- **Performance**: The effectiveness of the Chain of Agents depends on the quality of the summaries generated by the worker agents and the ability of the manager agent to synthesize the final output.
- **Latency**: Multiple sequential calls to the LLM API can result in higher latency compared to single-call approaches.

## Limitations

- The current implementation only supports text-based tasks.
- The effectiveness of the approach depends on the quality of the chunking strategy and the ability of worker agents to generate informative communication units.
- As the number of chunks increases, there might be some information loss in the chain.

## Future Improvements

- Support for multi-modal inputs and outputs
- Optimization of chunking strategies
- Fine-tuning of prompts for specific tasks
- Parallel processing of non-dependent chunks
- Integration with other LLM providers

## License

[MIT License](LICENSE)
